model:
  hidden_dim: 16
  input_size: 1
  n_layers: 1
  drop_prob: 0.25
  layer_norm: True

learning:
  batch_size: 512
  lr: 0.001
  epochs: 200
  grad_clip: 0.0
  
loss:
  T: 250

early_stopping:
  monitor: "val_loss"
  min_delta: 0
  patience: 10